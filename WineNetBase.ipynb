{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pcoh0DRtLUb3"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install sklearn\n",
    "# !pip install pyenchant\n",
    "# !apt-get install libenchant1c2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW_AxRSfLA4s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing import sequence\n",
    "import os \n",
    "# os.chdir('/content/drive/MyDrive/NeuralRes\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/MSCA_31009/Final_Project/')earch/wine-net/')\n",
    "\n",
    "def remove_punctuation(s):\n",
    "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "  # Removing punctuations in string\n",
    "  # Using loop + punctuation string\n",
    "  for ele in s:\n",
    "      if ele in punc:\n",
    "          s = s.replace(ele, \"\")\n",
    "  return s\n",
    "\n",
    "def clean_description(review, swap):\n",
    "  terms = review.split()\n",
    "  for x in terms:\n",
    "    try: x = swap[('t_' + str(x))]\n",
    "    except KeyError: x = ''\n",
    "  return ' '.join(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G1yVEY0j8lJP",
    "outputId": "7d502850-c68a-4c38-af2d-02e42f7b2a3e"
   },
   "outputs": [],
   "source": [
    "wines = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "wines = wines.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "wines2 = pd.read_csv('winemag-data_first150k.csv')\n",
    "wines2.head()\n",
    "\n",
    "# Intersect and append two review tables\n",
    "columns = [value for value in wines.columns if value in wines2.columns] # intersection(wines.columns, wines2.columns)\n",
    "wines = wines[columns]\n",
    "wines2 = wines2[columns]\n",
    "wines = pd.concat([wines, wines2]).drop(columns = ['designation', 'winery'])\n",
    "\n",
    "wines = wines.sample(n=20000)\n",
    "\n",
    "# Imputation steps\n",
    "median_price = wines.price.median()\n",
    "median_points = wines.points.median()\n",
    "\n",
    "# Impute variables\n",
    "wines.price = wines.price.fillna(median_price)\n",
    "wines.points = wines.points.fillna(median_points)\n",
    "wines = wines.fillna('')\n",
    "wines.price = wines.price.astype(int)\n",
    "wines.description = wines.description.apply(lambda x: remove_punctuation(x.lower()))\n",
    "\n",
    "Y = wines.variety\n",
    "output_shape = Y.nunique()\n",
    "Y0_price = wines.price\n",
    "Y1_points = wines.points.apply(lambda x: 1 if x > 90 else 0)\n",
    "wines = wines.drop(columns=['variety', 'price', 'points', 'region_1', 'region_2'])\n",
    "\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CITJKQneH4qN",
    "outputId": "3568e19a-8bf9-4119-8b09-a1d8b50bd5b7"
   },
   "outputs": [],
   "source": [
    "text = wines.description\n",
    "tfidf = TfidfVectorizer() \n",
    "review_vector = tfidf.fit_transform(text)\n",
    "words = tfidf.get_feature_names_out()\n",
    "rev_array = review_vector.toarray()\n",
    "words_df = pd.DataFrame(rev_array, columns = words)\n",
    "\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Qt04p9Z4L3uP",
    "outputId": "e0deab8e-7cee-463d-f73a-628bc8b4b2e5"
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "enchant.list_languages()\n",
    "eng = enchant.Dict(\"en_US\")\n",
    "drops = []\n",
    "for column in words_df.columns:\n",
    "    english = eng.check(column)\n",
    "    if any(map(str.isdigit, column)): \n",
    "        drops.append(column)\n",
    "    if not(english):\n",
    "        drops.append(column)\n",
    "\n",
    "words_df = words_df.drop(columns = drops)\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-Y4gvy9NQjib",
    "outputId": "81d6b5ba-293e-48a4-b897-eb664d7c790d"
   },
   "outputs": [],
   "source": [
    "# vectorizing the rest of the data in the table.\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZOhPeCRVP4p",
    "outputId": "9e853dec-8803-42bf-e121-59451ef1d9ae"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "X = wines.description.to_numpy()\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)\n",
    "X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arBV53PhWyvM"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# encode class values as integers\n",
    "print(Y1_points.nunique())\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y_processed = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAGNUZb1LUb9"
   },
   "outputs": [],
   "source": [
    "# words_df = words_df.reset_index()\n",
    "# wines = wines.reset_index()\n",
    "# wines = pd.concat([wines, words_df], axis=1)\n",
    "# wines.shape\n",
    "# wines = wines.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLi0A7iYLUb_"
   },
   "source": [
    "## Variety Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqANoxkMLUcA"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, Y_processed, test_size=0.25, random_state=0)\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "ffuQ3tq9LUcC",
    "outputId": "d727ca73-18f2-448b-ca49-93174f46476b"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Dense, LSTM, Embedding,Dropout,SpatialDropout1D,Conv1D,MaxPooling1D,GRU,BatchNormalization\n",
    "# from tensorflow.keras.layers import Input,Bidirectional,GlobalAveragePooling1D,GlobalMaxPooling1D,concatenate,LeakyReLU\n",
    "# from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=X_train.shape[0], output_dim=X_train.shape[1], weights=[X_train], input_length=max_log_length, trainable=False))\n",
    "# model.add(SpatialDropout1D(0.5))\n",
    "# model.add(Conv1D(filters=32, kernel_size=10 , kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Bidirectional(LSTM(units=64,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "# model.add(SpatialDropout1D(0.5))\n",
    "# model.add(Conv1D(filters=32, kernel_size=10 ,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Bidirectional(LSTM(lstm_units,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "# model.add(SpatialDropout1D(0.5))\n",
    "# model.add(Conv1D(filters=32, kernel_size=10 ,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Bidirectional(LSTM(lstm_units,dropout=0.5, recurrent_dropout=0.5)))\n",
    "# model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Dropout,SpatialDropout1D,Conv1D,MaxPooling1D,GRU,BatchNormalization\n",
    "from tensorflow.keras.layers import Input,Bidirectional,GlobalAveragePooling1D,GlobalMaxPooling1D,concatenate,LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=X_train.shape[0], output_dim=X_train.shape[1], weights=[X_train], input_length=max_log_length, trainable=False))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters=64, kernel_size=4 , kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters=64, kernel_size=4 ,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters=64, kernel_size=4 ,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Dense(1,activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2PN1akCLUcD"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NthpnfrYLUcD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaWNkR4pTRFc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WineNetBase.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
